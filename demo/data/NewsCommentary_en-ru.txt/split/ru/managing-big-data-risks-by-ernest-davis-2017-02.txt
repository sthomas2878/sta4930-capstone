<HEADLINE>
Как управлять большими рисками «больших данных»
<P>
НЬЮ-ЙОРК – В течение последних 15 лет мы стали свидетелями взрывного роста объёмов электронных данных (доступных из интернета, социальных сетей, научного оборудования, смартфонов, камер наблюдения и многих других источников), а также быстрого развития компьютерных технологий, применяемых для их обработки.
Эти так называемые «Большие данные» (Big Data), несомненно, помогут добиться важных научных, технических и медицинских успехов.
Однако «большие данные» создают ещё и серьёзные риски, если они используются неправильно или со злым умыслом.
<P>
В таких значительных инновациях, как например, поисковые системы в интернете, машинный перевод, маркировка изображений, уже активно применяются технологии машинного обучения для обработки огромных массивов данных.
А в ближайшем будущем «большие данные» позволят значительно улучшить качество принимаемых государственных решений, программ социальной помощи, научных знаний.
<P>
Однако наличие больших объёмов данных не может заменить собой наличие данных высокого качества.
Например, как отмечается в недавней статье в журнале Nature, организаторы предвыборных опросов в США испытывают серьёзные трудности с получением репрезентативной выборки населения, поскольку закон разрешает им звонить только на телефоны фиксированной, проводной связи, а между тем, американцы всё больше пользуются сотовой связью.
И хотя в социальных сетях можно найти бессчётное множество политических комментариев, их тоже нельзя считать надёжным отражением мнений избирателей.
Более того, значительная часть твитов и публикаций в Facebook на тему политики сгенерирована компьютерами.
<P>
В последние годы было множество скандалов, вызванных работой автоматизированных программ, опирающихся на тенденциозные массивы данных.
Например, когда в апреле прошлого года студентка одного из колледжей стала искать в Google картинки по запросу «непрофессиональные причёски для работы», в результатах поиска оказались в основном картинки чернокожих людей; когда же она заменила первое слово в запросе на «профессиональные», Google стал выдавать фотографии белых.
Однако такие результаты не были следствием тенденциозности программистов Google; они объясняются тем, что люди ставят подобные метки к картинкам в интернете.
<P>
Программа анализа «больших данных», применяемая для оценки решений о найме или повышении сотрудников, может, опираясь на подобные результаты поиска, отсеивать чернокожих кандидатов, которые похожи на тех, у кого «непрофессиональные причёски».
Тем самым, программа будет сохранять традиционные социальные предубеждения.
И это не просто гипотетическая вероятность.
В прошлом году исследование «моделей оценки рисков рецидива», проведённое ProPublica, показало, что широко используемая методология определения наказания для уголовных преступников, признанных виновными, систематически переоценивает вероятность совершения чернокожими подсудимыми новых преступлений в будущем и занижает риски подобных действий белыми обвиняемыми.
<P>
Другой риск «больших данных» связан с тем, что ими можно манипулировать.
Если люди знают, что при принятии важных решений, влияющих на их жизнь, используются массивы данных, у них появляется стимул склонить чашу весов в свою пользу.
Например, когда работу учителей оценивают по результатам их учеников на экзаменах, тогда они с большей вероятностью будут «учить ради экзаменов» или даже обманывать.
<P>
Точно так же руководители колледжей, желающие повысить место своего вуза в рейтинге журнала US News and World Reports, начинают совершать глупости, например, вкладывают деньги в экстравагантный спортзал, снизив расходы на преподавателей.
Хуже того, они начинают принимать чудовищно неэтичные решения, как например, Университет Маунт Сент-Мэри, где для повышения «коэффициента удержания студентов» стали выявлять и отчислять наиболее слабых студентов в первые несколько недель занятий.
<P>
Даже поисковый движок Google не обладает иммунитетом к манипуляциям.
Хотя он работает с колоссальными объёмами данных, а за его работой следят одни из самых лучших в мире специалистов по данным, его результаты поддаются «поисковой оптимизации» и различным методам манипуляции (поисковые бомбы, спамдексинг и так далее), которые служат неким узким интересам.
<P>
Третьей опасностью является нарушение конфиденциальности, поскольку очень многие из доступных сейчас данных содержат персональную информацию.
За последние годы с коммерческих и государственных сайтов были украдены огромные массивы конфиденциальных данных.
А как показали исследования, политические взгляды или даже сексуальную ориентацию человек можно довольно точно вычислить из весьма безобидных на первый взгляд онлайн-публикаций, например, рецензий на фильмы, причём даже если они опубликованы под псевдонимом.
<P>
Наконец, «большие данные» создают проблему ответственности.
Если человек чувствует, что решения алгоритма несправедливы к нему или к ней, у него зачастую нет никакой возможности пожаловаться на это: либо потому что конкретные результаты не поддаются интерпретации, либо потому что авторы алгоритмов отказываются раскрывать детали механизма их работы.
И хотя любого, кто недоволен, правительства и корпорации могут устрашать рассказами о том, какие их алгоритмы «математические» и «научные», они сами часто оказываются в ужасе от поведения своих созданий.
Евросоюз недавно принял решение предоставить людям, пострадавшим от алгоритмов, «право на объяснение», но лишь время покажет, как это всё будет работать на практике.
<P>
Если людям, которым «большие данные» нанесли вред, некуда обратиться за помощью, последствия могут оказаться очень токсичными и далекоидущими.
Это хорошо показано в новой книге Кэти О’Нил «Оружие математического поражения».
<P>
Хорошая новость в том, что угроз, создаваемых «большими данными», в основном можно избежать.
Но этого не случится, если мы не будем активно защищать право людей на конфиденциальность, находить и исправлять факты несправедливости, использовать рекомендации алгоритмов благоразумно, сохранять чёткое понимание внутренних механизмов работы алгоритмов, а также качества данных, на основании которых они принимают решения.
